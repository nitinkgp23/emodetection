{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/jasabanta/miniconda3/envs/dl/lib/python3.5/site-packages/sklearn/base.py:251: UserWarning: Trying to unpickle estimator LabelPropagation from version 0.18 when using version 0.20.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Input, merge, TimeDistributed, concatenate, Bidirectional\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import optimizers\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import backend as K\n",
    "import json, argparse, os, pickle\n",
    "import functools\n",
    "import re\n",
    "import io\n",
    "import sys\n",
    "from keras.layers.core import *\n",
    "from keras.models import *\n",
    "import normalise\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "import emoji\n",
    "import unicodedata\n",
    "\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "from ekphrasis.classes.spellcorrect import SpellCorrector\n",
    "from ekphrasis.classes.segmenter import Segmenter\n",
    "\n",
    "#from symspellpy.symspellpy import SymSpell, Verbosity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_capacity = 83000\n",
    "# # maximum edit distance per dictionary precalculation\n",
    "# max_edit_distance_dictionary = 10\n",
    "# prefix_length = 15\n",
    "# sym_spell = SymSpell(initial_capacity, max_edit_distance_dictionary,\n",
    "#                      prefix_length)\n",
    "# # load dictionary\n",
    "# dictionary_path = \"data/frequency_dictionary_en_82_765.txt\"\n",
    "# term_index = 0  # column of the term in the dictionary text file\n",
    "# count_index = 1  # column of the term frequency in the dictionary text file\n",
    "# if not sym_spell.load_dictionary(dictionary_path, term_index, count_index):\n",
    "#     print(\"Dictionary file not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_symspell(input_term):\n",
    "    \n",
    "    max_edit_distance_lookup = 2\n",
    "    suggestion_verbosity = Verbosity.CLOSEST  # TOP, CLOSEST, ALL\n",
    "    suggestions = sym_spell.lookup(input_term, suggestion_verbosity,\n",
    "                                   max_edit_distance_lookup)\n",
    "    return suggestions[0].term\n",
    "\n",
    "    '''\n",
    "    # lookup suggestions for multi-word input strings (supports compound\n",
    "    # splitting & merging)\n",
    "    input_term = (\"dont worry i'm girl <eos> hmm how do i know if you are <eos> whats ur name ?\")\n",
    "    # max edit distance per lookup (per single word, not per whole input string)\n",
    "    max_edit_distance_lookup = 2\n",
    "    suggestions = sym_spell.lookup_compound(input_term,\n",
    "                                            max_edit_distance_lookup)\n",
    "    # display suggestion term, edit distance, and term frequency\n",
    "    for suggestion in suggestions:\n",
    "        print(\"{}, {}, {}\".format(suggestion.term, suggestion.count,suggestion.distance))\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "          \"train_data_path\" : \"data/train.txt\",\n",
    "          \"test_data_path\" : \"data/devwithoutlabels.txt\",\n",
    "          \"solution_path\" : \"test.txt\",\n",
    "          \"embedding_matrix_path\" : \"data/embedding_conceptnet.npy\",   ## change this accordingly\n",
    "          \"emoji_dict_path\" : \"data/emoji_dict.txt\",\n",
    "          \"fast_text_embedding_path\" : \"data/wiki-news-300d-1M.vec\",\n",
    "          \"glove_embedding_path\" : \"data/glove.6B.100d.txt\",\n",
    "          \"conceptnet_path\" : \"data/conceptnetembed.txt\",\n",
    "          \"contractions_path\": 'data/contractions.json',\n",
    "          \"num_folds\" : 5,\n",
    "          \"num_classes\" : 4,\n",
    "          \"max_nb_words\" : 20000,\n",
    "          \"max_sequence_length\" : 100,\n",
    "          \"max_charsequence_length\": 15,\n",
    "          \"embedding_dim\" : 300,\n",
    "          \"batch_size\" : 256,\n",
    "          \"lstm_dim\" : 128,\n",
    "          \"learning_rate\" : 0.01,\n",
    "          \"dropout\" : 0.4,\n",
    "          \"num_epochs\" : 20\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2emotion = {0:\"others\", 1:\"happy\", 2: \"sad\", 3:\"angry\"}\n",
    "emotion2label = {\"others\":0, \"happy\":1, \"sad\":2, \"angry\":3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataPath = config[\"train_data_path\"]\n",
    "testDataPath = config[\"test_data_path\"]\n",
    "solutionPath = config[\"solution_path\"]\n",
    "embeddingMatrixPath = config[\"embedding_matrix_path\"]\n",
    "emojiDictPath = config[\"emoji_dict_path\"]\n",
    "fastTextEmbeddingPath = config[\"fast_text_embedding_path\"]\n",
    "gloveEmbeddingPath = config[\"glove_embedding_path\"]\n",
    "conceptnetEmbeddingPath = config[\"conceptnet_path\"]\n",
    "contractionsPath = config[\"contractions_path\"]\n",
    "NUM_FOLDS = config[\"num_folds\"]\n",
    "NUM_CLASSES = config[\"num_classes\"]\n",
    "MAX_NB_WORDS = config[\"max_nb_words\"]\n",
    "MAX_SEQUENCE_LENGTH = config[\"max_sequence_length\"]\n",
    "MAX_CHARSEQUENCE_LENGTH = config[\"max_charsequence_length\"]\n",
    "EMBEDDING_DIM = config[\"embedding_dim\"]\n",
    "BATCH_SIZE = config[\"batch_size\"]\n",
    "LSTM_DIM = config[\"lstm_dim\"]\n",
    "DROPOUT = config[\"dropout\"]\n",
    "LEARNING_RATE = config[\"learning_rate\"]\n",
    "NUM_EPOCHS = config[\"num_epochs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(contractionsPath) as f:\n",
    "    contractions = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n",
      "Reading english - 2grams ...\n",
      "Reading english - 1grams ...\n",
      "Reading english - 1grams ...\n",
      "Reading english - 1grams ...\n",
      "Reading english - 2grams ...\n"
     ]
    }
   ],
   "source": [
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "        'time', 'url', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    #annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "    #    'emphasis', 'censored'},\n",
    "    #annotate = {\"allcaps\"},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    #segmenter=\"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    #corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=True,  # spell correction for elongated words\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    ")\n",
    "\n",
    "sp = SpellCorrector(corpus=\"english\") \n",
    "# wordnet_lemmatizer = WordNetLemmatizer()\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "seg_eng = Segmenter(corpus=\"english\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ekphrasis_preprocess(sent):\n",
    "    # spell correct\n",
    "    t = text_processor.pre_process_doc(sent)\n",
    "    t = [sp.correct(word) for word in t if word not in string.punctuation]\n",
    "    \n",
    "    # lemmatize\n",
    "#     to_add = []\n",
    "#     for i in t:\n",
    "#         if i not in stop_words:\n",
    "#             to_add.append(wordnet_lemmatizer.lemmatize(i))\n",
    "    new_sent = \" \".join(t)\n",
    "\n",
    "    \n",
    "    return new_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = [\n",
    "#     \"CANT WAIT for the new season of #TwinPeaks ＼(^o^)／!!! #davidlynch #tvseries :)))\",\n",
    "#     \"I saw the new #johndoe movie and it suuuuucks!!! WAISTED $10... #badmovies :/\",\n",
    "#     \"@SentimentSymp:  can't wait for the Nov 9 #Sentiment talks!  YAAAAAAY !!! :-D http://sentimentsymposium.com/.\"\n",
    "# ]\n",
    "\n",
    "# for s in sentences:\n",
    "#     print(ekphrasis_preprocess(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessData(dataFilePath, mode):\n",
    "    \"\"\"Load data from a file, process and return indices, conversations and labels in separate lists\n",
    "    Input:\n",
    "        dataFilePath : Path to train/test file to be processed\n",
    "        mode : \"train\" mode returns labels. \"test\" mode doesn't return labels.\n",
    "    Output:\n",
    "        indices : Unique conversation ID list\n",
    "        conversations : List of 3 turn conversations, processed and each turn separated by the <eos> tag\n",
    "        labels : [Only available in \"train\" mode] List of labels\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    conversations = []\n",
    "    labels = []\n",
    "    with io.open(dataFilePath, encoding=\"utf8\") as finput:\n",
    "        finput.readline()\n",
    "        for line in finput:\n",
    "            #print(line)\n",
    "            count = 0\n",
    "            # Convert multiple instances of . ? ! , to single instance\n",
    "            # okay...sure -> okay . sure\n",
    "            # okay???sure -> okay ? sure\n",
    "            # Add whitespace around such punctuation\n",
    "            # okay!sure -> okay ! sure\n",
    "            repeatedChars = ['.', '?', '!', ',']\n",
    "            for c in repeatedChars:\n",
    "                lineSplit = line.split(c)\n",
    "                while True:\n",
    "                    try:\n",
    "                        lineSplit.remove('')\n",
    "                    except:\n",
    "                        break\n",
    "                cSpace = ' ' + c + ' '    \n",
    "                line = cSpace.join(lineSplit)\n",
    "            \n",
    "            line = line.strip().split('\\t')\n",
    "            if mode == \"train\":\n",
    "                # Train data contains id, 3 turns and label\n",
    "                label = emotion2label[line[4]]\n",
    "                labels.append(label)\n",
    "            \n",
    "            emoji_dict = json.load(open(emojiDictPath))         \n",
    "       \n",
    "            # Expand abbreviations, \n",
    "            full_expanded = []\n",
    "            for c in line[1:4]:\n",
    "                #replace emojis\n",
    "                d = c\n",
    "                for character in c:\n",
    "                    if character in emoji.UNICODE_EMOJI:\n",
    "                        #print(c)\n",
    "                        uni = 'U+' + hex(ord(character))[2:].upper()\n",
    "                        d = d.replace(character, ' '+emoji_dict[uni]+' ')\n",
    "\n",
    "                expanded = []\n",
    "                words = d.split()\n",
    "                for word in words:\n",
    "                    word = word.replace(\"’\",\"'\")  # difference in apostrophe's\n",
    "                    word = seg_eng.segment(word)  # segmentation\n",
    "                    ex = contractions.get(word.lower())\n",
    "                    if not ex:\n",
    "                        ex = word\n",
    "                    expanded.append(ex)\n",
    "                full_expanded.append(ekphrasis_preprocess(' '.join(expanded)))  # ekphrasis preprocess\n",
    "        \n",
    "            conv = ' <eos> '.join(full_expanded)            \n",
    "            #print(conv)\n",
    "            # Remove any duplicate spaces\n",
    "            duplicateSpacePattern = re.compile(r'\\ +')\n",
    "            conv = re.sub(duplicateSpacePattern, ' ', conv)\n",
    "            \n",
    "            indices.append(int(line[0]))\n",
    "            conversations.append(conv.lower())\n",
    "            \n",
    "            count+=1\n",
    "            if count > 20:\n",
    "                break\n",
    "\n",
    "    if mode == \"train\":\n",
    "        return indices, conversations, labels\n",
    "    else:\n",
    "        return indices, conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training data...\n",
      "Processing test data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing training data...\")\n",
    "trainIndices, trainTexts, labels_pre = preprocessData(trainDataPath, mode=\"train\")\n",
    "# Write normalised text to file to check if normalisation works. Disabled now. Uncomment following line to enable   \n",
    "# writeNormalisedData(trainDataPath, trainTexts)\n",
    "print(\"Processing test data...\")\n",
    "testIndices, testTexts = preprocessData(testDataPath, mode=\"test\")\n",
    "# writeNormalisedData(testDataPath, testTexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddingMatrix(wordIndex):\n",
    "    \"\"\"Populate an embedding matrix using a word-index. If the word \"happy\" has an index 19,\n",
    "       the 19th row in the embedding matrix should contain the embedding vector for the word \"happy\".\n",
    "    Input:\n",
    "        wordIndex : A dictionary of (word : index) pairs, extracted using a tokeniser\n",
    "    Output:\n",
    "        embeddingMatrix : A matrix where every row has 100 dimensional GloVe embedding\n",
    "    \"\"\"\n",
    "    embeddingsIndex = {}\n",
    "    # Load the embedding vectors from ther GloVe file\n",
    "    #with io.open(gloveEmbedidngPath), encoding=\"utf8\") as f:\\\n",
    "    with io.open((conceptnetEmbeddingPath), encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            embeddingVector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddingsIndex[word] = embeddingVector\n",
    "    \n",
    "    print('Found %s word vectors.' % len(embeddingsIndex))\n",
    "    \n",
    "    # Minimum word index of any word is 1. \n",
    "    embeddingMatrix = np.zeros((len(wordIndex) + 1, EMBEDDING_DIM))\n",
    "    for word, i in wordIndex.items():\n",
    "        embeddingVector = embeddingsIndex.get(word)\n",
    "        if embeddingVector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embeddingMatrix[i] = embeddingVector\n",
    "    \n",
    "    return embeddingMatrix\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tokens...\n",
      "Found 12891 unique tokens.\n",
      "Populating embedding matrix...\n",
      "Found 417195 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting tokens...\")\n",
    "tokenizer_word = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer_word.fit_on_texts(trainTexts)\n",
    "trainSequences = tokenizer_word.texts_to_sequences(trainTexts)\n",
    "testSequences = tokenizer_word.texts_to_sequences(testTexts)\n",
    "\n",
    "wordIndex = tokenizer_word.word_index\n",
    "print(\"Found %s unique tokens.\" % len(wordIndex))\n",
    "\n",
    "print(\"Populating embedding matrix...\")\n",
    "embeddingMatrix = getEmbeddingMatrix(wordIndex)\n",
    "#embeddingMatrix = np.load(embeddingMatrixPath)\n",
    "t = np.where(~embeddingMatrix.any(axis=1))[0]\n",
    "np.save(embeddingMatrixPath, embeddingMatrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tokens/ characters...\n",
      "Found 61 unique charactertokens.\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting tokens/ characters...\")\n",
    "tokenizer_char = Tokenizer(num_words=MAX_NB_WORDS, char_level=True)\n",
    "tokenizer_char.fit_on_texts(trainTexts)\n",
    "charIndex = tokenizer_char.word_index\n",
    "charIndex['PAD'] = 0\n",
    "charIndex['UNK'] = len(charIndex)\n",
    "trainSequences_char = []\n",
    "testSequences_char = []\n",
    "\n",
    "for s in trainTexts:\n",
    "    sentence = []\n",
    "    for w in s.split(' '):\n",
    "        #if w is not '<eos>':\n",
    "        word = [charIndex[x] for x in w]\n",
    "        sentence.append(word)\n",
    "    trainSequences_char.append(sentence)\n",
    "\n",
    "for s in testTexts:\n",
    "    sentence = []\n",
    "    for w in s.split(' '):\n",
    "        #if w is not '<eos>':\n",
    "        word = [charIndex.get(x, charIndex['UNK']) for x in w]\n",
    "        sentence.append(word)\n",
    "    testSequences_char.append(sentence)\n",
    "\n",
    "print(\"Found %s unique charactertokens.\" % len(charIndex))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# typos = []\n",
    "# for _ in t:\n",
    "#     for k, v in wordIndex.items():\n",
    "#         if _ == v:\n",
    "#               typos.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fuzzywuzzy import fuzz\n",
    "\n",
    "# fuzz.partial_ratio('yesssssssssss','yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(typos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMetrics(predictions, ground):\n",
    "    \"\"\"Given predicted labels and the respective ground truth labels, display some metrics\n",
    "    Input: shape [# of samples, NUM_CLASSES]\n",
    "        predictions : Model output. Every row has 4 decimal values, with the highest belonging to the predicted class\n",
    "        ground : Ground truth labels, converted to one-hot encodings. A sample belonging to Happy class will be [0, 1, 0, 0]\n",
    "    Output:\n",
    "        accuracy : Average accuracy\n",
    "        microPrecision : Precision calculated on a micro level. Ref - https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin/16001\n",
    "        microRecall : Recall calculated on a micro level\n",
    "        microF1 : Harmonic mean of microPrecision and microRecall. Higher value implies better classification  \n",
    "    \"\"\"\n",
    "    # [0.1, 0.3 , 0.2, 0.1] -> [0, 1, 0, 0]\n",
    "    discretePredictions = to_categorical(predictions.argmax(axis=1))\n",
    "    \n",
    "    truePositives = np.sum(discretePredictions*ground, axis=0)\n",
    "    falsePositives = np.sum(np.clip(discretePredictions - ground, 0, 1), axis=0)\n",
    "    falseNegatives = np.sum(np.clip(ground-discretePredictions, 0, 1), axis=0)\n",
    "    \n",
    "    print(\"True Positives per class : \", truePositives)\n",
    "    print(\"False Positives per class : \", falsePositives)\n",
    "    print(\"False Negatives per class : \", falseNegatives)\n",
    "    \n",
    "    # ------------- Macro level calculation ---------------\n",
    "    macroPrecision = 0\n",
    "    macroRecall = 0\n",
    "    \n",
    "    # We ignore the \"Others\" class during the calculation of Precision, Recall and F1\n",
    "    for c in range(1, NUM_CLASSES):\n",
    "        precision = truePositives[c] / (truePositives[c] + falsePositives[c])\n",
    "        macroPrecision += precision\n",
    "        recall = truePositives[c] / (truePositives[c] + falseNegatives[c])\n",
    "        macroRecall += recall\n",
    "        f1 = ( 2 * recall * precision ) / (precision + recall) if (precision+recall) > 0 else 0\n",
    "        print(\"Class %s : Precision : %.3f, Recall : %.3f, F1 : %.3f\" % (label2emotion[c], precision, recall, f1))\n",
    "    \n",
    "    macroPrecision /= 3\n",
    "    macroRecall /= 3\n",
    "    macroF1 = (2 * macroRecall * macroPrecision ) / (macroPrecision + macroRecall) if (macroPrecision+macroRecall) > 0 else 0\n",
    "    print(\"Ignoring the Others class, Macro Precision : %.4f, Macro Recall : %.4f, Macro F1 : %.4f\" % (macroPrecision, macroRecall, macroF1))   \n",
    "    \n",
    "    # ------------- Micro level calculation ---------------\n",
    "    truePositives = truePositives[1:].sum()\n",
    "    falsePositives = falsePositives[1:].sum()\n",
    "    falseNegatives = falseNegatives[1:].sum()    \n",
    "    \n",
    "    print(\"Ignoring the Others class, Micro TP : %d, FP : %d, FN : %d\" % (truePositives, falsePositives, falseNegatives))\n",
    "    \n",
    "    microPrecision = truePositives / (truePositives + falsePositives)\n",
    "    microRecall = truePositives / (truePositives + falseNegatives)\n",
    "    \n",
    "    microF1 = ( 2 * microRecall * microPrecision ) / (microPrecision + microRecall) if (microPrecision+microRecall) > 0 else 0\n",
    "    # -----------------------------------------------------\n",
    "    \n",
    "    predictions = predictions.argmax(axis=1)\n",
    "    ground = ground.argmax(axis=1)\n",
    "    accuracy = np.mean(predictions==ground)\n",
    "    \n",
    "    print(\"Accuracy : %.4f, Micro Precision : %.4f, Micro Recall : %.4f, Micro F1 : %.4f\" % (accuracy, microPrecision, microRecall, microF1))\n",
    "    return accuracy, microPrecision, microRecall, microF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeNormalisedData(dataFilePath, texts):\n",
    "    \"\"\"Write normalised data to a file\n",
    "    Input:\n",
    "        dataFilePath : Path to original train/test file that has been processed\n",
    "        texts : List containing the normalised 3 turn conversations, separated by the <eos> tag.\n",
    "    \"\"\"\n",
    "    normalisedDataFilePath = dataFilePath.replace(\".txt\", \"_normalised.txt\")\n",
    "    with io.open(normalisedDataFilePath, 'w', encoding='utf8') as fout:\n",
    "        with io.open(dataFilePath, encoding='utf8') as fin:\n",
    "            fin.readline()\n",
    "            for lineNum, line in enumerate(fin):\n",
    "                line = line.strip().split('\\t')\n",
    "                normalisedLine = texts[lineNum].strip().split('<eos>')\n",
    "                fout.write(line[0] + '\\t')\n",
    "                # Write the original turn, followed by the normalised version of the same turn\n",
    "                fout.write(line[1] + '\\t' + normalisedLine[0] + '\\t')\n",
    "                fout.write(line[2] + '\\t' + normalisedLine[1] + '\\t')\n",
    "                fout.write(line[3] + '\\t' + normalisedLine[2] + '\\t')\n",
    "                try:\n",
    "                    # If label information available (train time)\n",
    "                    fout.write(line[4] + '\\n')    \n",
    "                except:\n",
    "                    # If label information not available (test time)\n",
    "                    fout.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_keras_metric(method):\n",
    "    @functools.wraps(method)\n",
    "    def wrapper(self, args, **kwargs):\n",
    "        \"\"\" Wrapper for turning tensorflow metrics into keras metrics \"\"\"\n",
    "        value, update_op = method(self, args, **kwargs)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([update_op]):\n",
    "            value = tf.identity(value)\n",
    "        return value\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel(char_output_dim = 10,\n",
    "               char_dropout=0.5,\n",
    "               char_lstm_units=40, \n",
    "               main_lstm_units = LSTM_DIM, \n",
    "               main_dropout = DROPOUT):\n",
    "    \"\"\"Constructs the architecture of the model\n",
    "    Input:\n",
    "        embeddingMatrix : The embedding matrix to be loaded in the embedding layer.\n",
    "    Output:\n",
    "        model : A basic LSTM model\n",
    "    \"\"\"\n",
    "    word_in = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "\n",
    "    \n",
    "    word_embedding = Embedding(embeddingMatrix.shape[0],\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embeddingMatrix],\n",
    "                                #input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)(word_in)\n",
    "    \n",
    "    n_chars = len(charIndex)\n",
    "    # input and embeddings for characters\n",
    "    char_in = Input(shape=(MAX_SEQUENCE_LENGTH, MAX_CHARSEQUENCE_LENGTH,))\n",
    "    emb_char = TimeDistributed(Embedding(input_dim=n_chars,\n",
    "                                         output_dim=char_output_dim,\n",
    "                                         input_length=MAX_CHARSEQUENCE_LENGTH,\n",
    "                                         mask_zero=True))(char_in)\n",
    "    \n",
    "    # character LSTM to get word encodings by characters\n",
    "    char_embedding = TimeDistributed(Bidirectional(LSTM(units=char_lstm_units, return_sequences=False,\n",
    "                                    recurrent_dropout=char_dropout)))(emb_char)\n",
    "\n",
    "    \n",
    "    embeddingLayer = concatenate([word_embedding, char_embedding])\n",
    "    \n",
    "    lstmLayer = Bidirectional(LSTM(units = main_lstm_units, dropout=main_dropout))(embeddingLayer)\n",
    "    predictions = Dense(NUM_CLASSES, activation='sigmoid')(lstmLayer)\n",
    "    \n",
    "    model = Model(inputs=[word_in, char_in], outputs=predictions)\n",
    "    \n",
    "#     model = Sequential()\n",
    "#     model.add(embeddingLayer)\n",
    "#     model.add(LSTM(LSTM_DIM, dropout=DROPOUT))\n",
    "#     model.add(Dense(NUM_CLASSES, activation='sigmoid'))\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    precision = as_keras_metric(tf.metrics.precision)\n",
    "    recall = as_keras_metric(tf.metrics.recall)\n",
    "    \n",
    "    rmsprop = optimizers.rmsprop(lr=LEARNING_RATE)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=rmsprop,\n",
    "                  metrics=['acc', precision, recall])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModelbase():\n",
    "    word_in = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "\n",
    "    \n",
    "    word_embedding = Embedding(embeddingMatrix.shape[0],\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embeddingMatrix],\n",
    "                                #input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)(word_in)\n",
    "    \n",
    "    lstmLayer = BiLSTM(LSTM_DIM, dropout=DROPOUT)(word_embedding)\n",
    "    predictions = Dense(NUM_CLASSES, activation='sigmoid')(lstmLayer)\n",
    "    \n",
    "    model = Model(inputs=word_in, outputs=predictions)\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    rmsprop = optimizers.rmsprop(lr=LEARNING_RATE)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=rmsprop,\n",
    "                  metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 100, 15)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 100, 15, 10)  610         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 100, 300)     3867600     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 100, 80)      16320       time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 100, 380)     0           embedding_2[0][0]                \n",
      "                                                                 time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 256)          521216      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 4)            1028        bidirectional_2[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 4,406,774\n",
      "Trainable params: 539,174\n",
      "Non-trainable params: 3,867,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x7f71f0556a58>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buildModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pad_sequences(sequences, sequences_char):\n",
    "    wordseq = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH,\n",
    "                             padding='post', truncating='post')\n",
    "    charseq_pre = []\n",
    "    for seq in sequences_char:\n",
    "        padded_seq = pad_sequences(seq, maxlen=MAX_CHARSEQUENCE_LENGTH,\n",
    "                                   padding='post', truncating='post')\n",
    "        charseq_pre.append(padded_seq)\n",
    "    charseq_pre = np.array(charseq_pre)\n",
    "    charseq = pad_sequences(charseq_pre, maxlen=MAX_SEQUENCE_LENGTH,\n",
    "                                 padding='post', truncating='post')\n",
    "    return [wordseq, charseq]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting k-fold cross validation...\n",
      "----------------------------------------\n",
      "Fold 1/5\n",
      "Building model...\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 100, 15)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 100, 15, 15)  915         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 100, 300)     3867600     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 100, 96)      24576       time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 100, 396)     0           embedding_4[0][0]                \n",
      "                                                                 time_distributed_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 320)          712960      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 4)            1284        bidirectional_4[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 4,607,335\n",
      "Trainable params: 739,735\n",
      "Non-trainable params: 3,867,600\n",
      "__________________________________________________________________________________________________\n",
      "Train on 24128 samples, validate on 6032 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-433e4e6185ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m               \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m               verbose=2)\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxVal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "[data_wordseq,data_charseq] = create_pad_sequences(trainSequences, trainSequences_char)\n",
    "labels = to_categorical(np.asarray(labels_pre))\n",
    "\n",
    "# Randomize data\n",
    "np.random.seed(10)\n",
    "np.random.shuffle(trainIndices)\n",
    "# lim = int(0.85*len(trainSequences))\n",
    "data_wordseq = data_wordseq[trainIndices]\n",
    "data_charseq = data_charseq[trainIndices]\n",
    "labels_train = labels[trainIndices]\n",
    "\n",
    "data_train = [data_wordseq, data_charseq]\n",
    "\n",
    "cod = 15\n",
    "clu = 48\n",
    "mlu = 160\n",
    "\n",
    "# Perform k-fold cross validation\n",
    "metrics = {\"accuracy\" : [],\n",
    "           \"microPrecision\" : [],\n",
    "           \"microRecall\" : [],\n",
    "           \"microF1\" : []}\n",
    "\n",
    "print(\"Starting k-fold cross validation...\")\n",
    "for k in range(3):\n",
    "    print('-'*40)\n",
    "    print(\"Fold {}/{}\".format(k+1, NUM_FOLDS))\n",
    "    validationSize = int(len(labels_train)/NUM_FOLDS)\n",
    "    index1 = validationSize * k\n",
    "    index2 = validationSize * (k+1)\n",
    "\n",
    "    xTrain_word = np.vstack((data_wordseq[:index1],data_wordseq[index2:]))\n",
    "    xTrain_char = np.vstack((data_charseq[:index1],data_charseq[index2:]))\n",
    "    yTrain = np.vstack((labels_train[:index1],labels_train[index2:]))\n",
    "    xTrain = [xTrain_word, xTrain_char]\n",
    "\n",
    "    xVal_word = data_wordseq[index1:index2]\n",
    "    xVal_char = data_charseq[index1:index2]\n",
    "    yVal = labels_train[index1:index2]\n",
    "    xVal = [xVal_word, xVal_char]\n",
    "\n",
    "    \n",
    "    print(\"Building model...\")\n",
    "    model = buildModel(char_output_dim=cod,\n",
    "                       char_lstm_units=clu,\n",
    "                       main_lstm_units=mlu\n",
    "                      )\n",
    "    model.fit(xTrain, yTrain, \n",
    "              validation_data=(xVal, yVal),\n",
    "              epochs=NUM_EPOCHS,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              verbose=2)\n",
    "\n",
    "    predictions = model.predict(xVal, batch_size=BATCH_SIZE)\n",
    "    accuracy, microPrecision, microRecall, microF1 = getMetrics(predictions, yVal)\n",
    "    metrics[\"accuracy\"].append(accuracy)\n",
    "    metrics[\"microPrecision\"].append(microPrecision)\n",
    "    metrics[\"microRecall\"].append(microRecall)\n",
    "    metrics[\"microF1\"].append(microF1)\n",
    "\n",
    "print(\"\\n============= Metrics =================\")\n",
    "print(\"Average Cross-Validation Accuracy : \")\n",
    "print(\"%.4f\" % (sum(metrics[\"accuracy\"])/len(metrics[\"accuracy\"])))\n",
    "print(\"Average Cross-Validation Micro Precision :\")\n",
    "print(\"%.4f\" % (sum(metrics[\"microPrecision\"])/len(metrics[\"microPrecision\"])))\n",
    "print(\"Average Cross-Validation Micro Recall :\")\n",
    "print(\"%.4f\" % (sum(metrics[\"microRecall\"])/len(metrics[\"microRecall\"])))\n",
    "print(\"Average Cross-Validation Micro F1 :\")\n",
    "print(\"%.4f\" % (sum(metrics[\"microF1\"])/len(metrics[\"microF1\"])))\n",
    "\n",
    "print(\"\\n======================================\")\n",
    "\n",
    "print(\"Retraining model on entire data to create solution file\")\n",
    "\n",
    "\n",
    "model = buildModel(char_output_dim=cod,\n",
    "                   char_lstm_units=clu,\n",
    "                   main_lstm_units=mlu\n",
    "                  )\n",
    "history = model.fit(data_train, labels_train, \n",
    "                    epochs = int(NUM_EPOCHS),\n",
    "                    batch_size = BATCH_SIZE,\n",
    "                    verbose=2)\n",
    "\n",
    "#model.save('EP%d_LR%de-5_LDim%d_BS%d.h5'%(NUM_EPOCHS, int(LEARNING_RATE*(10**5)), LSTM_DIM, BATCH_SIZE))\n",
    "#model = load_model('EP%d_LR%de-5_LDim%d_BS%d.h5'%(20, int(LEARNING_RATE*(10**5)), LSTM_DIM, BATCH_SIZE))\n",
    "\n",
    "print(\"Creating solution file...\")\n",
    "testData = create_pad_sequences(testSequences, testSequences_char)\n",
    "predictions = model.predict(testData, batch_size=BATCH_SIZE)\n",
    "predictions = predictions.argmax(axis=1)\n",
    "\n",
    "with io.open(solutionPath, \"w\", encoding=\"utf8\") as fout:\n",
    "    fout.write('\\t'.join([\"id\", \"turn1\", \"turn2\", \"turn3\", \"label\"]) + '\\n')        \n",
    "    with io.open(testDataPath, encoding=\"utf8\") as fin:\n",
    "        fin.readline()\n",
    "        for lineNum, line in enumerate(fin):\n",
    "            fout.write('\\t'.join(line.strip().split('\\t')[:4]) + '\\t')\n",
    "            fout.write(label2emotion[predictions[lineNum]] + '\\n')\n",
    "print(\"Completed. Model parameters: \")\n",
    "print(\"Learning rate : %.3f, LSTM Dim : %d, Dropout : %.3f, Batch_size : %d\" \n",
    "      % (LEARNING_RATE, LSTM_DIM, DROPOUT, BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
