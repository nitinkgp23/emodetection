{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/kalpitchittora/miniconda3/envs/dl/lib/python3.5/site-packages/sklearn/base.py:251: UserWarning: Trying to unpickle estimator LabelPropagation from version 0.18 when using version 0.20.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Input, merge, TimeDistributed, concatenate, Bidirectional\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "import json, argparse, os\n",
    "import re\n",
    "import io\n",
    "import sys\n",
    "from keras.layers.core import *\n",
    "from keras.models import *\n",
    "import normalise\n",
    "\n",
    "import emoji\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2emotion = {0:\"others\", 1:\"happy\", 2: \"sad\", 3:\"angry\"}\n",
    "emotion2label = {\"others\":0, \"happy\":1, \"sad\":2, \"angry\":3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "          \"train_data_path\" : \"data/train.txt\",\n",
    "          \"test_data_path\" : \"data/devwithoutlabels.txt\",\n",
    "          \"solution_path\" : \"test.txt\",\n",
    "          \"embedding_matrix_path\" : \"data/embedding_conceptnet.npy\",   ## change this accordingly\n",
    "          \"emoji_dict_path\" : \"data/emoji_dict.txt\",\n",
    "          \"fast_text_embedding_path\" : \"data/wiki-news-300d-1M.vec\",\n",
    "          \"glove_embedding_path\" : \"data/glove.6B.100d.txt\",\n",
    "          \"conceptnet_path\" : \"data/conceptnetembed.txt\",\n",
    "          \"contractions_path\": 'data/contractions.json',\n",
    "          \"num_folds\" : 5,\n",
    "          \"num_classes\" : 4,\n",
    "          \"max_nb_words\" : 20000,\n",
    "          \"max_sequence_length\" : 100,\n",
    "          \"max_charsequence_length\": 15,\n",
    "          \"embedding_dim\" : 300,\n",
    "          \"batch_size\" : 256,\n",
    "          \"lstm_dim\" : 128,\n",
    "          \"learning_rate\" : 0.01,\n",
    "          \"dropout\" : 0.4,\n",
    "          \"num_epochs\" : 20\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataPath = config[\"train_data_path\"]\n",
    "testDataPath = config[\"test_data_path\"]\n",
    "solutionPath = config[\"solution_path\"]\n",
    "embeddingMatrixPath = config[\"embedding_matrix_path\"]\n",
    "emojiDictPath = config[\"emoji_dict_path\"]\n",
    "fastTextEmbeddingPath = config[\"fast_text_embedding_path\"]\n",
    "gloveEmbeddingPath = config[\"glove_embedding_path\"]\n",
    "conceptnetEmbeddingPath = config[\"conceptnet_path\"]\n",
    "contractionsPath = config[\"contractions_path\"]\n",
    "NUM_FOLDS = config[\"num_folds\"]\n",
    "NUM_CLASSES = config[\"num_classes\"]\n",
    "MAX_NB_WORDS = config[\"max_nb_words\"]\n",
    "MAX_SEQUENCE_LENGTH = config[\"max_sequence_length\"]\n",
    "MAX_CHARSEQUENCE_LENGTH = config[\"max_charsequence_length\"]\n",
    "EMBEDDING_DIM = config[\"embedding_dim\"]\n",
    "BATCH_SIZE = config[\"batch_size\"]\n",
    "LSTM_DIM = config[\"lstm_dim\"]\n",
    "DROPOUT = config[\"dropout\"]\n",
    "LEARNING_RATE = config[\"learning_rate\"]\n",
    "NUM_EPOCHS = config[\"num_epochs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(contractionsPath) as f:\n",
    "    contractions = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessData(dataFilePath, mode):\n",
    "    \"\"\"Load data from a file, process and return indices, conversations and labels in separate lists\n",
    "    Input:\n",
    "        dataFilePath : Path to train/test file to be processed\n",
    "        mode : \"train\" mode returns labels. \"test\" mode doesn't return labels.\n",
    "    Output:\n",
    "        indices : Unique conversation ID list\n",
    "        conversations : List of 3 turn conversations, processed and each turn separated by the <eos> tag\n",
    "        labels : [Only available in \"train\" mode] List of labels\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    conversations = []\n",
    "    labels = []\n",
    "    with io.open(dataFilePath, encoding=\"utf8\") as finput:\n",
    "        finput.readline()\n",
    "        for line in finput:\n",
    "            #print(line)\n",
    "            count = 0\n",
    "            # Convert multiple instances of . ? ! , to single instance\n",
    "            # okay...sure -> okay . sure\n",
    "            # okay???sure -> okay ? sure\n",
    "            # Add whitespace around such punctuation\n",
    "            # okay!sure -> okay ! sure\n",
    "            repeatedChars = ['.', '?', '!', ',']\n",
    "            for c in repeatedChars:\n",
    "                lineSplit = line.split(c)\n",
    "                while True:\n",
    "                    try:\n",
    "                        lineSplit.remove('')\n",
    "                    except:\n",
    "                        break\n",
    "                cSpace = ' ' + c + ' '    \n",
    "                line = cSpace.join(lineSplit)\n",
    "            \n",
    "            line = line.strip().split('\\t')\n",
    "            if mode == \"train\":\n",
    "                # Train data contains id, 3 turns and label\n",
    "                label = emotion2label[line[4]]\n",
    "                labels.append(label)\n",
    "            \n",
    "            emoji_dict = json.load(open(emojiDictPath))         \n",
    "       \n",
    "            # Expand abbreviations, \n",
    "            full_expanded = []\n",
    "            for c in line[1:4]:\n",
    "                #replace emojis\n",
    "                d = c\n",
    "                for character in c:\n",
    "                    if character in emoji.UNICODE_EMOJI:\n",
    "                        #print(c)\n",
    "                        uni = 'U+' + hex(ord(character))[2:].upper()\n",
    "                        d = d.replace(character, ' '+emoji_dict[uni]+' ')\n",
    "                \n",
    "                expanded = []\n",
    "                words = d.split()\n",
    "                for word in words:\n",
    "                    word = word.replace(\"â€™\",\"'\")  # difference in apostrophe's\n",
    "                    ex = contractions.get(word.lower())\n",
    "                    if not ex:\n",
    "                        ex = word\n",
    "                    expanded.append(ex)\n",
    "                full_expanded.append(' '.join(expanded))\n",
    "            \n",
    "            conv = ' <eos> '.join(full_expanded)\n",
    "            #print(conv)\n",
    "            # Remove any duplicate spaces\n",
    "            duplicateSpacePattern = re.compile(r'\\ +')\n",
    "            conv = re.sub(duplicateSpacePattern, ' ', conv)\n",
    "            \n",
    "            indices.append(int(line[0]))\n",
    "            conversations.append(conv.lower())\n",
    "\n",
    "    if mode == \"train\":\n",
    "        return indices, conversations, labels\n",
    "    else:\n",
    "        return indices, conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training data...\n",
      "Processing test data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing training data...\")\n",
    "trainIndices, trainTexts, labels_pre = preprocessData(trainDataPath, mode=\"train\")\n",
    "# Write normalised text to file to check if normalisation works. Disabled now. Uncomment following line to enable   \n",
    "# writeNormalisedData(trainDataPath, trainTexts)\n",
    "print(\"Processing test data...\")\n",
    "testIndices, testTexts = preprocessData(testDataPath, mode=\"test\")\n",
    "# writeNormalisedData(testDataPath, testTexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddingMatrix(wordIndex):\n",
    "    \"\"\"Populate an embedding matrix using a word-index. If the word \"happy\" has an index 19,\n",
    "       the 19th row in the embedding matrix should contain the embedding vector for the word \"happy\".\n",
    "    Input:\n",
    "        wordIndex : A dictionary of (word : index) pairs, extracted using a tokeniser\n",
    "    Output:\n",
    "        embeddingMatrix : A matrix where every row has 100 dimensional GloVe embedding\n",
    "    \"\"\"\n",
    "    embeddingsIndex = {}\n",
    "    # Load the embedding vectors from ther GloVe file\n",
    "    #with io.open(gloveEmbedidngPath), encoding=\"utf8\") as f:\\\n",
    "    with io.open((fastTextEmbeddingPath), encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            embeddingVector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddingsIndex[word] = embeddingVector\n",
    "    \n",
    "    print('Found %s word vectors.' % len(embeddingsIndex))\n",
    "    \n",
    "    # Minimum word index of any word is 1. \n",
    "    embeddingMatrix = np.zeros((len(wordIndex) + 1, EMBEDDING_DIM))\n",
    "    for word, i in wordIndex.items():\n",
    "        embeddingVector = embeddingsIndex.get(word)\n",
    "        if embeddingVector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embeddingMatrix[i] = embeddingVector\n",
    "    \n",
    "    return embeddingMatrix\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tokens...\n",
      "Found 14838 unique tokens.\n",
      "Populating embedding matrix...\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting tokens...\")\n",
    "tokenizer_word = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer_word.fit_on_texts(trainTexts)\n",
    "trainSequences = tokenizer_word.texts_to_sequences(trainTexts)\n",
    "testSequences = tokenizer_word.texts_to_sequences(testTexts)\n",
    "\n",
    "wordIndex = tokenizer_word.word_index\n",
    "print(\"Found %s unique tokens.\" % len(wordIndex))\n",
    "\n",
    "print(\"Populating embedding matrix...\")\n",
    "# embeddingMatrix = getEmbeddingMatrix(wordIndex)\n",
    "\n",
    "# t = np.where(~embeddingMatrix.any(axis=1))[0]\n",
    "# np.save(embeddingMatrixPath, embeddingMatrix)\n",
    "embeddingMatrix = np.load(embeddingMatrixPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tokens/ characters...\n",
      "Found 134 unique charactertokens.\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting tokens/ characters...\")\n",
    "tokenizer_char = Tokenizer(num_words=MAX_NB_WORDS, char_level=True)\n",
    "tokenizer_char.fit_on_texts(trainTexts)\n",
    "charIndex = tokenizer_char.word_index\n",
    "charIndex['PAD'] = 0\n",
    "charIndex['UNK'] = len(charIndex)\n",
    "trainSequences_char = []\n",
    "testSequences_char = []\n",
    "\n",
    "for s in trainTexts:\n",
    "    sentence = []\n",
    "    for w in s.split(' '):\n",
    "        #if w is not '<eos>':\n",
    "        word = [charIndex[x] for x in w]\n",
    "        sentence.append(word)\n",
    "    trainSequences_char.append(sentence)\n",
    "\n",
    "for s in testTexts:\n",
    "    sentence = []\n",
    "    for w in s.split(' '):\n",
    "        #if w is not '<eos>':\n",
    "        word = [charIndex.get(x, charIndex['UNK']) for x in w]\n",
    "        sentence.append(word)\n",
    "    testSequences_char.append(sentence)\n",
    "\n",
    "print(\"Found %s unique charactertokens.\" % len(charIndex))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMetrics(predictions, ground):\n",
    "    \"\"\"Given predicted labels and the respective ground truth labels, display some metrics\n",
    "    Input: shape [# of samples, NUM_CLASSES]\n",
    "        predictions : Model output. Every row has 4 decimal values, with the highest belonging to the predicted class\n",
    "        ground : Ground truth labels, converted to one-hot encodings. A sample belonging to Happy class will be [0, 1, 0, 0]\n",
    "    Output:\n",
    "        accuracy : Average accuracy\n",
    "        microPrecision : Precision calculated on a micro level. Ref - https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin/16001\n",
    "        microRecall : Recall calculated on a micro level\n",
    "        microF1 : Harmonic mean of microPrecision and microRecall. Higher value implies better classification  \n",
    "    \"\"\"\n",
    "    # [0.1, 0.3 , 0.2, 0.1] -> [0, 1, 0, 0]\n",
    "    discretePredictions = to_categorical(predictions.argmax(axis=1))\n",
    "    \n",
    "    truePositives = np.sum(discretePredictions*ground, axis=0)\n",
    "    falsePositives = np.sum(np.clip(discretePredictions - ground, 0, 1), axis=0)\n",
    "    falseNegatives = np.sum(np.clip(ground-discretePredictions, 0, 1), axis=0)\n",
    "    \n",
    "    print(\"True Positives per class : \", truePositives)\n",
    "    print(\"False Positives per class : \", falsePositives)\n",
    "    print(\"False Negatives per class : \", falseNegatives)\n",
    "    \n",
    "    # ------------- Macro level calculation ---------------\n",
    "    macroPrecision = 0\n",
    "    macroRecall = 0\n",
    "    \n",
    "    # We ignore the \"Others\" class during the calculation of Precision, Recall and F1\n",
    "    for c in range(1, NUM_CLASSES):\n",
    "        precision = truePositives[c] / (truePositives[c] + falsePositives[c])\n",
    "        macroPrecision += precision\n",
    "        recall = truePositives[c] / (truePositives[c] + falseNegatives[c])\n",
    "        macroRecall += recall\n",
    "        f1 = ( 2 * recall * precision ) / (precision + recall) if (precision+recall) > 0 else 0\n",
    "        print(\"Class %s : Precision : %.3f, Recall : %.3f, F1 : %.3f\" % (label2emotion[c], precision, recall, f1))\n",
    "    \n",
    "    macroPrecision /= 3\n",
    "    macroRecall /= 3\n",
    "    macroF1 = (2 * macroRecall * macroPrecision ) / (macroPrecision + macroRecall) if (macroPrecision+macroRecall) > 0 else 0\n",
    "    print(\"Ignoring the Others class, Macro Precision : %.4f, Macro Recall : %.4f, Macro F1 : %.4f\" % (macroPrecision, macroRecall, macroF1))   \n",
    "    \n",
    "    # ------------- Micro level calculation ---------------\n",
    "    truePositives = truePositives[1:].sum()\n",
    "    falsePositives = falsePositives[1:].sum()\n",
    "    falseNegatives = falseNegatives[1:].sum()    \n",
    "    \n",
    "    print(\"Ignoring the Others class, Micro TP : %d, FP : %d, FN : %d\" % (truePositives, falsePositives, falseNegatives))\n",
    "    \n",
    "    microPrecision = truePositives / (truePositives + falsePositives)\n",
    "    microRecall = truePositives / (truePositives + falseNegatives)\n",
    "    \n",
    "    microF1 = ( 2 * microRecall * microPrecision ) / (microPrecision + microRecall) if (microPrecision+microRecall) > 0 else 0\n",
    "    # -----------------------------------------------------\n",
    "    \n",
    "    predictions = predictions.argmax(axis=1)\n",
    "    ground = ground.argmax(axis=1)\n",
    "    accuracy = np.mean(predictions==ground)\n",
    "    \n",
    "    print(\"Accuracy : %.4f, Micro Precision : %.4f, Micro Recall : %.4f, Micro F1 : %.4f\" % (accuracy, microPrecision, microRecall, microF1))\n",
    "    return accuracy, microPrecision, microRecall, microF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeNormalisedData(dataFilePath, texts):\n",
    "    \"\"\"Write normalised data to a file\n",
    "    Input:\n",
    "        dataFilePath : Path to original train/test file that has been processed\n",
    "        texts : List containing the normalised 3 turn conversations, separated by the <eos> tag.\n",
    "    \"\"\"\n",
    "    normalisedDataFilePath = dataFilePath.replace(\".txt\", \"_normalised.txt\")\n",
    "    with io.open(normalisedDataFilePath, 'w', encoding='utf8') as fout:\n",
    "        with io.open(dataFilePath, encoding='utf8') as fin:\n",
    "            fin.readline()\n",
    "            for lineNum, line in enumerate(fin):\n",
    "                line = line.strip().split('\\t')\n",
    "                normalisedLine = texts[lineNum].strip().split('<eos>')\n",
    "                fout.write(line[0] + '\\t')\n",
    "                # Write the original turn, followed by the normalised version of the same turn\n",
    "                fout.write(line[1] + '\\t' + normalisedLine[0] + '\\t')\n",
    "                fout.write(line[2] + '\\t' + normalisedLine[1] + '\\t')\n",
    "                fout.write(line[3] + '\\t' + normalisedLine[2] + '\\t')\n",
    "                try:\n",
    "                    # If label information available (train time)\n",
    "                    fout.write(line[4] + '\\n')    \n",
    "                except:\n",
    "                    # If label information not available (test time)\n",
    "                    fout.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(inputs, attention_size, time_major=False, return_alphas=False):\n",
    "    \"\"\"\n",
    "    Attention mechanism layer which reduces RNN/Bi-RNN outputs with Attention vector.\n",
    "    The idea was proposed in the article by Z. Yang et al., \"Hierarchical Attention Networks\n",
    "     for Document Classification\", 2016: http://www.aclweb.org/anthology/N16-1174.\n",
    "    Variables notation is also inherited from the article\n",
    "    Args:\n",
    "        inputs: The Attention inputs.\n",
    "            Matches outputs of RNN/Bi-RNN layer (not final state):\n",
    "                In case of RNN, this must be RNN outputs `Tensor`:\n",
    "                    If time_major == False (default), this must be a tensor of shape:\n",
    "                        `[batch_size, max_time, cell.output_size]`.\n",
    "                    If time_major == True, this must be a tensor of shape:\n",
    "                        `[max_time, batch_size, cell.output_size]`.\n",
    "                In case of Bidirectional RNN, this must be a tuple (outputs_fw, outputs_bw) containing the forward and\n",
    "                the backward RNN outputs `Tensor`.\n",
    "                    If time_major == False (default),\n",
    "                        outputs_fw is a `Tensor` shaped:\n",
    "                        `[batch_size, max_time, cell_fw.output_size]`\n",
    "                        and outputs_bw is a `Tensor` shaped:\n",
    "                        `[batch_size, max_time, cell_bw.output_size]`.\n",
    "                    If time_major == True,\n",
    "                        outputs_fw is a `Tensor` shaped:\n",
    "                        `[max_time, batch_size, cell_fw.output_size]`\n",
    "                        and outputs_bw is a `Tensor` shaped:\n",
    "                        `[max_time, batch_size, cell_bw.output_size]`.\n",
    "        attention_size: Linear size of the Attention weights.\n",
    "        time_major: The shape format of the `inputs` Tensors.\n",
    "            If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n",
    "            If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n",
    "            Using `time_major = True` is a bit more efficient because it avoids\n",
    "            transposes at the beginning and end of the RNN calculation.  However,\n",
    "            most TensorFlow data is batch-major, so by default this function\n",
    "            accepts input and emits output in batch-major form.\n",
    "        return_alphas: Whether to return attention coefficients variable along with layer's output.\n",
    "            Used for visualization purpose.\n",
    "    Returns:\n",
    "        The Attention output `Tensor`.\n",
    "        In case of RNN, this will be a `Tensor` shaped:\n",
    "            `[batch_size, cell.output_size]`.\n",
    "        In case of Bidirectional RNN, this will be a `Tensor` shaped:\n",
    "            `[batch_size, cell_fw.output_size + cell_bw.output_size]`.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(inputs, tuple):\n",
    "        # In case of Bi-RNN, concatenate the forward and the backward RNN outputs.\n",
    "        inputs = tf.concat(inputs, 2)\n",
    "\n",
    "    if time_major:\n",
    "        # (T,B,D) => (B,T,D)\n",
    "        inputs = tf.array_ops.transpose(inputs, [1, 0, 2])\n",
    "\n",
    "    hidden_size = inputs.shape[2].value  # D value - hidden size of the RNN layer\n",
    "\n",
    "    # Trainable parameters\n",
    "    w_omega = tf.Variable(tf.random_normal([hidden_size, attention_size], stddev=0.1))\n",
    "    b_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "    u_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "\n",
    "    with tf.name_scope('v'):\n",
    "        # Applying fully connected layer with non-linear activation to each of the B*T timestamps;\n",
    "        #  the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size\n",
    "        v = tf.tanh(tf.tensordot(inputs, w_omega, axes=1) + b_omega)\n",
    "\n",
    "    # For each of the timestamps its vector of size A from `v` is reduced with `u` vector\n",
    "    vu = tf.tensordot(v, u_omega, axes=1, name='vu')  # (B,T) shape\n",
    "    alphas = tf.nn.softmax(vu, name='alphas')  # (B,T) shape\n",
    "\n",
    "    # Output of (Bi-)RNN is reduced with attention vector; the result has (B,D) shape\n",
    "    output = keras.backend.sum(inputs * tf.expand_dims(alphas, -1), 1)\n",
    "\n",
    "    if not return_alphas:\n",
    "        return output\n",
    "    else:\n",
    "        return output, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel():\n",
    "    word_in = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "\n",
    "    \n",
    "    word_embedding = Embedding(embeddingMatrix.shape[0],\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embeddingMatrix],\n",
    "                                #input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)(word_in)\n",
    "    \n",
    "    lstmLayer = Bidirectional(LSTM(LSTM_DIM, dropout=DROPOUT,return_sequences=True))(word_embedding)\n",
    "    attention_layer = attention(lstmLayer, 5)\n",
    "    print(attention_layer)\n",
    "    print(lstmLayer)\n",
    "    predictions = Dense(NUM_CLASSES, activation='sigmoid')(lstmLayer)\n",
    "    \n",
    "    model = Model(inputs=word_in, outputs=predictions)\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    rmsprop = optimizers.rmsprop(lr=LEARNING_RATE)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=rmsprop,\n",
    "                  metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-21cd63316148>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbuildModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-15ab40dba20d>\u001b[0m in \u001b[0;36mbuildModel\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mlstmLayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDROPOUT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mattention_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstmLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstmLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-2948a3a3f03c>\u001b[0m in \u001b[0;36mattention\u001b[0;34m(inputs, attention_size, time_major, return_alphas)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m# Output of (Bi-)RNN is reduced with attention vector; the result has (B,D) shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_alphas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "buildModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pad_sequences(sequences, sequences_char=None):\n",
    "    wordseq = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH,\n",
    "                             padding='post', truncating='post')\n",
    "    if not sequences_char:\n",
    "        return wordseq\n",
    "    \n",
    "    charseq_pre = []\n",
    "    for seq in sequences_char:\n",
    "        padded_seq = pad_sequences(seq, maxlen=MAX_CHARSEQUENCE_LENGTH,\n",
    "                                   padding='post', truncating='post')\n",
    "        charseq_pre.append(padded_seq)\n",
    "    charseq_pre = np.array(charseq_pre)\n",
    "    charseq = pad_sequences(charseq_pre, maxlen=MAX_SEQUENCE_LENGTH,\n",
    "                                 padding='post', truncating='post')\n",
    "    return [wordseq, charseq]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[data_wordseq,data_charseq] = create_pad_sequences(trainSequences, trainSequences_char)\n",
    "data_wordseq = create_pad_sequences(trainSequences)\n",
    "labels = to_categorical(np.asarray(labels_pre))\n",
    "\n",
    "\n",
    "# Randomize data\n",
    "np.random.seed(10)\n",
    "np.random.shuffle(trainIndices)\n",
    "# lim = int(0.85*len(trainSequences))\n",
    "data_wordseq = data_wordseq[trainIndices]\n",
    "#data_charseq = data_charseq[trainIndices]\n",
    "labels_train = labels[trainIndices]\n",
    "\n",
    "#data_train = [data_wordseq, data_charseq]\n",
    "data_train = data_wordseq\n",
    "\n",
    "# Perform k-fold cross validation\n",
    "metrics = {\"accuracy\" : [],\n",
    "           \"microPrecision\" : [],\n",
    "           \"microRecall\" : [],\n",
    "           \"microF1\" : []}\n",
    "\n",
    "print(\"Starting k-fold cross validation...\")\n",
    "for k in range(3):\n",
    "    print('-'*40)\n",
    "    print(\"Fold %d/%d\" % (k+1, NUM_FOLDS))\n",
    "    validationSize = int(len(labels_train)/NUM_FOLDS)\n",
    "    index1 = validationSize * k\n",
    "    index2 = validationSize * (k+1)\n",
    "\n",
    "    xTrain = np.vstack((data_train[:index1],data_train[index2:]))\n",
    "    yTrain = np.vstack((labels[:index1],labels[index2:]))\n",
    "    xVal = data_train[index1:index2]\n",
    "    yVal = labels[index1:index2]\n",
    "    print(\"Building model...\")\n",
    "    model = buildModel()\n",
    "    model.fit(xTrain,\n",
    "              yTrain, \n",
    "              validation_data=(xVal, yVal),\n",
    "              epochs=NUM_EPOCHS,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              verbose=2)\n",
    "\n",
    "    predictions = model.predict(xVal, batch_size=BATCH_SIZE)\n",
    "    accuracy, microPrecision, microRecall, microF1 = getMetrics(predictions, yVal)\n",
    "    metrics[\"accuracy\"].append(accuracy)\n",
    "    metrics[\"microPrecision\"].append(microPrecision)\n",
    "    metrics[\"microRecall\"].append(microRecall)\n",
    "    metrics[\"microF1\"].append(microF1)\n",
    "\n",
    "print(\"\\n============= Metrics =================\")\n",
    "print(\"Average Cross-Validation Accuracy : \")\n",
    "print(\"%.4f\" % (sum(metrics[\"accuracy\"])/len(metrics[\"accuracy\"])))\n",
    "print(\"Average Cross-Validation Micro Precision :\")\n",
    "print(\"%.4f\" % (sum(metrics[\"microPrecision\"])/len(metrics[\"microPrecision\"])))\n",
    "print(\"Average Cross-Validation Micro Recall :\")\n",
    "print(\"%.4f\" % (sum(metrics[\"microRecall\"])/len(metrics[\"microRecall\"])))\n",
    "print(\"Average Cross-Validation Micro F1 :\")\n",
    "print(\"%.4f\" % (sum(metrics[\"microF1\"])/len(metrics[\"microF1\"])))\n",
    "\n",
    "print(\"\\n======================================\")\n",
    "\n",
    "print(\"Retraining model on entire data to create solution file\")\n",
    "model = buildModel()\n",
    "history = model.fit(data_train,\n",
    "                    labels_train,\n",
    "                    epochs=int(NUM_EPOCHS),\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    validation_data=(data_valid, labels_valid),\n",
    "                    verbose=2\n",
    "                   )\n",
    "#model.save('EP%d_LR%de-5_LDim%d_BS%d.h5'%(NUM_EPOCHS, int(LEARNING_RATE*(10**5)), LSTM_DIM, BATCH_SIZE))\n",
    "#model = load_model('EP%d_LR%de-5_LDim%d_BS%d.h5'%(20, int(LEARNING_RATE*(10**5)), LSTM_DIM, BATCH_SIZE))\n",
    "\n",
    "print(\"Creating solution file...\")\n",
    "# testData = create_pad_sequences(testSequences, testSequences_char)\n",
    "testData = create_pad_sequences(testSequences)\n",
    "\n",
    "predictions = model.predict(testData, batch_size=BATCH_SIZE)\n",
    "predictions = predictions.argmax(axis=1)\n",
    "\n",
    "with io.open(solutionPath, \"w\", encoding=\"utf8\") as fout:\n",
    "    fout.write('\\t'.join([\"id\", \"turn1\", \"turn2\", \"turn3\", \"label\"]) + '\\n')        \n",
    "    with io.open(testDataPath, encoding=\"utf8\") as fin:\n",
    "        fin.readline()\n",
    "        for lineNum, line in enumerate(fin):\n",
    "            fout.write('\\t'.join(line.strip().split('\\t')[:4]) + '\\t')\n",
    "            fout.write(label2emotion[predictions[lineNum]] + '\\n')\n",
    "print(\"Completed. Model parameters: \")\n",
    "print(\"Learning rate : %.3f, LSTM Dim : %d, Dropout : %.3f, Batch_size : %d\" \n",
    "      % (LEARNING_RATE, LSTM_DIM, DROPOUT, BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl]",
   "language": "python",
   "name": "conda-env-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
