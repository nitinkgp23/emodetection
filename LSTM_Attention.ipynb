{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/kalpitchittora/miniconda3/envs/dl/lib/python3.5/site-packages/sklearn/base.py:251: UserWarning: Trying to unpickle estimator LabelPropagation from version 0.18 when using version 0.20.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Input, merge, TimeDistributed, concatenate, Bidirectional\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "import json, argparse, os\n",
    "import re\n",
    "import io\n",
    "import sys\n",
    "from keras.layers.core import *\n",
    "from keras.models import *\n",
    "import normalise\n",
    "\n",
    "import emoji\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2emotion = {0:\"others\", 1:\"happy\", 2: \"sad\", 3:\"angry\"}\n",
    "emotion2label = {\"others\":0, \"happy\":1, \"sad\":2, \"angry\":3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "          \"train_data_path\" : \"data/train.txt\",\n",
    "          \"test_data_path\" : \"data/devwithoutlabels.txt\",\n",
    "          \"solution_path\" : \"test.txt\",\n",
    "          \"embedding_matrix_path\" : \"data/embedding_conceptnet.npy\",   ## change this accordingly\n",
    "          \"emoji_dict_path\" : \"data/emoji_dict.txt\",\n",
    "          \"fast_text_embedding_path\" : \"data/wiki-news-300d-1M.vec\",\n",
    "          \"glove_embedding_path\" : \"data/glove.6B.100d.txt\",\n",
    "          \"conceptnet_path\" : \"data/conceptnetembed.txt\",\n",
    "          \"contractions_path\": 'data/contractions.json',\n",
    "          \"num_folds\" : 5,\n",
    "          \"num_classes\" : 4,\n",
    "          \"max_nb_words\" : 20000,\n",
    "          \"max_sequence_length\" : 100,\n",
    "          \"max_charsequence_length\": 15,\n",
    "          \"embedding_dim\" : 300,\n",
    "          \"batch_size\" : 256,\n",
    "          \"lstm_dim\" : 128,\n",
    "          \"learning_rate\" : 0.01,\n",
    "          \"dropout\" : 0.4,\n",
    "          \"num_epochs\" : 20\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataPath = config[\"train_data_path\"]\n",
    "testDataPath = config[\"test_data_path\"]\n",
    "solutionPath = config[\"solution_path\"]\n",
    "embeddingMatrixPath = config[\"embedding_matrix_path\"]\n",
    "emojiDictPath = config[\"emoji_dict_path\"]\n",
    "fastTextEmbeddingPath = config[\"fast_text_embedding_path\"]\n",
    "gloveEmbeddingPath = config[\"glove_embedding_path\"]\n",
    "conceptnetEmbeddingPath = config[\"conceptnet_path\"]\n",
    "contractionsPath = config[\"contractions_path\"]\n",
    "NUM_FOLDS = config[\"num_folds\"]\n",
    "NUM_CLASSES = config[\"num_classes\"]\n",
    "MAX_NB_WORDS = config[\"max_nb_words\"]\n",
    "MAX_SEQUENCE_LENGTH = config[\"max_sequence_length\"]\n",
    "MAX_CHARSEQUENCE_LENGTH = config[\"max_charsequence_length\"]\n",
    "EMBEDDING_DIM = config[\"embedding_dim\"]\n",
    "BATCH_SIZE = config[\"batch_size\"]\n",
    "LSTM_DIM = config[\"lstm_dim\"]\n",
    "DROPOUT = config[\"dropout\"]\n",
    "LEARNING_RATE = config[\"learning_rate\"]\n",
    "NUM_EPOCHS = config[\"num_epochs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(contractionsPath) as f:\n",
    "    contractions = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessData(dataFilePath, mode):\n",
    "    \"\"\"Load data from a file, process and return indices, conversations and labels in separate lists\n",
    "    Input:\n",
    "        dataFilePath : Path to train/test file to be processed\n",
    "        mode : \"train\" mode returns labels. \"test\" mode doesn't return labels.\n",
    "    Output:\n",
    "        indices : Unique conversation ID list\n",
    "        conversations : List of 3 turn conversations, processed and each turn separated by the <eos> tag\n",
    "        labels : [Only available in \"train\" mode] List of labels\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    conversations = []\n",
    "    labels = []\n",
    "    with io.open(dataFilePath, encoding=\"utf8\") as finput:\n",
    "        finput.readline()\n",
    "        for line in finput:\n",
    "            #print(line)\n",
    "            count = 0\n",
    "            # Convert multiple instances of . ? ! , to single instance\n",
    "            # okay...sure -> okay . sure\n",
    "            # okay???sure -> okay ? sure\n",
    "            # Add whitespace around such punctuation\n",
    "            # okay!sure -> okay ! sure\n",
    "            repeatedChars = ['.', '?', '!', ',']\n",
    "            for c in repeatedChars:\n",
    "                lineSplit = line.split(c)\n",
    "                while True:\n",
    "                    try:\n",
    "                        lineSplit.remove('')\n",
    "                    except:\n",
    "                        break\n",
    "                cSpace = ' ' + c + ' '    \n",
    "                line = cSpace.join(lineSplit)\n",
    "            \n",
    "            line = line.strip().split('\\t')\n",
    "            if mode == \"train\":\n",
    "                # Train data contains id, 3 turns and label\n",
    "                label = emotion2label[line[4]]\n",
    "                labels.append(label)\n",
    "            \n",
    "            emoji_dict = json.load(open(emojiDictPath))         \n",
    "       \n",
    "            # Expand abbreviations, \n",
    "            full_expanded = []\n",
    "            for c in line[1:4]:\n",
    "                #replace emojis\n",
    "                d = c\n",
    "                for character in c:\n",
    "                    if character in emoji.UNICODE_EMOJI:\n",
    "                        #print(c)\n",
    "                        uni = 'U+' + hex(ord(character))[2:].upper()\n",
    "                        d = d.replace(character, ' '+emoji_dict[uni]+' ')\n",
    "                \n",
    "                expanded = []\n",
    "                words = d.split()\n",
    "                for word in words:\n",
    "                    word = word.replace(\"’\",\"'\")  # difference in apostrophe's\n",
    "                    ex = contractions.get(word.lower())\n",
    "                    if not ex:\n",
    "                        ex = word\n",
    "                    expanded.append(ex)\n",
    "                full_expanded.append(' '.join(expanded))\n",
    "            \n",
    "            conv = ' <eos> '.join(full_expanded)\n",
    "            #print(conv)\n",
    "            # Remove any duplicate spaces\n",
    "            duplicateSpacePattern = re.compile(r'\\ +')\n",
    "            conv = re.sub(duplicateSpacePattern, ' ', conv)\n",
    "            \n",
    "            indices.append(int(line[0]))\n",
    "            conversations.append(conv.lower())\n",
    "\n",
    "    if mode == \"train\":\n",
    "        return indices, conversations, labels\n",
    "    else:\n",
    "        return indices, conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training data...\n",
      "Processing test data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing training data...\")\n",
    "trainIndices, trainTexts, labels_pre = preprocessData(trainDataPath, mode=\"train\")\n",
    "# Write normalised text to file to check if normalisation works. Disabled now. Uncomment following line to enable   \n",
    "# writeNormalisedData(trainDataPath, trainTexts)\n",
    "print(\"Processing test data...\")\n",
    "testIndices, testTexts = preprocessData(testDataPath, mode=\"test\")\n",
    "# writeNormalisedData(testDataPath, testTexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddingMatrix(wordIndex):\n",
    "    \"\"\"Populate an embedding matrix using a word-index. If the word \"happy\" has an index 19,\n",
    "       the 19th row in the embedding matrix should contain the embedding vector for the word \"happy\".\n",
    "    Input:\n",
    "        wordIndex : A dictionary of (word : index) pairs, extracted using a tokeniser\n",
    "    Output:\n",
    "        embeddingMatrix : A matrix where every row has 100 dimensional GloVe embedding\n",
    "    \"\"\"\n",
    "    embeddingsIndex = {}\n",
    "    # Load the embedding vectors from ther GloVe file\n",
    "    #with io.open(gloveEmbedidngPath), encoding=\"utf8\") as f:\\\n",
    "    with io.open((conceptnetEmbeddingPath), encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            embeddingVector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddingsIndex[word] = embeddingVector\n",
    "    \n",
    "    print('Found %s word vectors.' % len(embeddingsIndex))\n",
    "    \n",
    "    # Minimum word index of any word is 1. \n",
    "    embeddingMatrix = np.zeros((len(wordIndex) + 1, EMBEDDING_DIM))\n",
    "    for word, i in wordIndex.items():\n",
    "        embeddingVector = embeddingsIndex.get(word)\n",
    "        if embeddingVector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embeddingMatrix[i] = embeddingVector\n",
    "    \n",
    "    return embeddingMatrix\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tokens...\n",
      "Found 14838 unique tokens.\n",
      "Populating embedding matrix...\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting tokens...\")\n",
    "tokenizer_word = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer_word.fit_on_texts(trainTexts)\n",
    "trainSequences = tokenizer_word.texts_to_sequences(trainTexts)\n",
    "testSequences = tokenizer_word.texts_to_sequences(testTexts)\n",
    "\n",
    "wordIndex = tokenizer_word.word_index\n",
    "print(\"Found %s unique tokens.\" % len(wordIndex))\n",
    "\n",
    "print(\"Populating embedding matrix...\")\n",
    "# embeddingMatrix = getEmbeddingMatrix(wordIndex)\n",
    "\n",
    "# t = np.where(~embeddingMatrix.any(axis=1))[0]\n",
    "# np.save(embeddingMatrixPath, embeddingMatrix)\n",
    "embeddingMatrix = np.load(embeddingMatrixPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tokens/ characters...\n",
      "Found 134 unique charactertokens.\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting tokens/ characters...\")\n",
    "tokenizer_char = Tokenizer(num_words=MAX_NB_WORDS, char_level=True)\n",
    "tokenizer_char.fit_on_texts(trainTexts)\n",
    "charIndex = tokenizer_char.word_index\n",
    "charIndex['PAD'] = 0\n",
    "charIndex['UNK'] = len(charIndex)\n",
    "trainSequences_char = []\n",
    "testSequences_char = []\n",
    "\n",
    "for s in trainTexts:\n",
    "    sentence = []\n",
    "    for w in s.split(' '):\n",
    "        #if w is not '<eos>':\n",
    "        word = [charIndex[x] for x in w]\n",
    "        sentence.append(word)\n",
    "    trainSequences_char.append(sentence)\n",
    "\n",
    "for s in testTexts:\n",
    "    sentence = []\n",
    "    for w in s.split(' '):\n",
    "        #if w is not '<eos>':\n",
    "        word = [charIndex.get(x, charIndex['UNK']) for x in w]\n",
    "        sentence.append(word)\n",
    "    testSequences_char.append(sentence)\n",
    "\n",
    "print(\"Found %s unique charactertokens.\" % len(charIndex))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMetrics(predictions, ground):\n",
    "    \"\"\"Given predicted labels and the respective ground truth labels, display some metrics\n",
    "    Input: shape [# of samples, NUM_CLASSES]\n",
    "        predictions : Model output. Every row has 4 decimal values, with the highest belonging to the predicted class\n",
    "        ground : Ground truth labels, converted to one-hot encodings. A sample belonging to Happy class will be [0, 1, 0, 0]\n",
    "    Output:\n",
    "        accuracy : Average accuracy\n",
    "        microPrecision : Precision calculated on a micro level. Ref - https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin/16001\n",
    "        microRecall : Recall calculated on a micro level\n",
    "        microF1 : Harmonic mean of microPrecision and microRecall. Higher value implies better classification  \n",
    "    \"\"\"\n",
    "    # [0.1, 0.3 , 0.2, 0.1] -> [0, 1, 0, 0]\n",
    "    discretePredictions = to_categorical(predictions.argmax(axis=1))\n",
    "    \n",
    "    truePositives = np.sum(discretePredictions*ground, axis=0)\n",
    "    falsePositives = np.sum(np.clip(discretePredictions - ground, 0, 1), axis=0)\n",
    "    falseNegatives = np.sum(np.clip(ground-discretePredictions, 0, 1), axis=0)\n",
    "    \n",
    "    print(\"True Positives per class : \", truePositives)\n",
    "    print(\"False Positives per class : \", falsePositives)\n",
    "    print(\"False Negatives per class : \", falseNegatives)\n",
    "    \n",
    "    # ------------- Macro level calculation ---------------\n",
    "    macroPrecision = 0\n",
    "    macroRecall = 0\n",
    "    \n",
    "    # We ignore the \"Others\" class during the calculation of Precision, Recall and F1\n",
    "    for c in range(1, NUM_CLASSES):\n",
    "        precision = truePositives[c] / (truePositives[c] + falsePositives[c])\n",
    "        macroPrecision += precision\n",
    "        recall = truePositives[c] / (truePositives[c] + falseNegatives[c])\n",
    "        macroRecall += recall\n",
    "        f1 = ( 2 * recall * precision ) / (precision + recall) if (precision+recall) > 0 else 0\n",
    "        print(\"Class %s : Precision : %.3f, Recall : %.3f, F1 : %.3f\" % (label2emotion[c], precision, recall, f1))\n",
    "    \n",
    "    macroPrecision /= 3\n",
    "    macroRecall /= 3\n",
    "    macroF1 = (2 * macroRecall * macroPrecision ) / (macroPrecision + macroRecall) if (macroPrecision+macroRecall) > 0 else 0\n",
    "    print(\"Ignoring the Others class, Macro Precision : %.4f, Macro Recall : %.4f, Macro F1 : %.4f\" % (macroPrecision, macroRecall, macroF1))   \n",
    "    \n",
    "    # ------------- Micro level calculation ---------------\n",
    "    truePositives = truePositives[1:].sum()\n",
    "    falsePositives = falsePositives[1:].sum()\n",
    "    falseNegatives = falseNegatives[1:].sum()    \n",
    "    \n",
    "    print(\"Ignoring the Others class, Micro TP : %d, FP : %d, FN : %d\" % (truePositives, falsePositives, falseNegatives))\n",
    "    \n",
    "    microPrecision = truePositives / (truePositives + falsePositives)\n",
    "    microRecall = truePositives / (truePositives + falseNegatives)\n",
    "    \n",
    "    microF1 = ( 2 * microRecall * microPrecision ) / (microPrecision + microRecall) if (microPrecision+microRecall) > 0 else 0\n",
    "    # -----------------------------------------------------\n",
    "    \n",
    "    predictions = predictions.argmax(axis=1)\n",
    "    ground = ground.argmax(axis=1)\n",
    "    accuracy = np.mean(predictions==ground)\n",
    "    \n",
    "    print(\"Accuracy : %.4f, Micro Precision : %.4f, Micro Recall : %.4f, Micro F1 : %.4f\" % (accuracy, microPrecision, microRecall, microF1))\n",
    "    return accuracy, microPrecision, microRecall, microF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeNormalisedData(dataFilePath, texts):\n",
    "    \"\"\"Write normalised data to a file\n",
    "    Input:\n",
    "        dataFilePath : Path to original train/test file that has been processed\n",
    "        texts : List containing the normalised 3 turn conversations, separated by the <eos> tag.\n",
    "    \"\"\"\n",
    "    normalisedDataFilePath = dataFilePath.replace(\".txt\", \"_normalised.txt\")\n",
    "    with io.open(normalisedDataFilePath, 'w', encoding='utf8') as fout:\n",
    "        with io.open(dataFilePath, encoding='utf8') as fin:\n",
    "            fin.readline()\n",
    "            for lineNum, line in enumerate(fin):\n",
    "                line = line.strip().split('\\t')\n",
    "                normalisedLine = texts[lineNum].strip().split('<eos>')\n",
    "                fout.write(line[0] + '\\t')\n",
    "                # Write the original turn, followed by the normalised version of the same turn\n",
    "                fout.write(line[1] + '\\t' + normalisedLine[0] + '\\t')\n",
    "                fout.write(line[2] + '\\t' + normalisedLine[1] + '\\t')\n",
    "                fout.write(line[3] + '\\t' + normalisedLine[2] + '\\t')\n",
    "                try:\n",
    "                    # If label information available (train time)\n",
    "                    fout.write(line[4] + '\\n')    \n",
    "                except:\n",
    "                    # If label information not available (test time)\n",
    "                    fout.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(inputs, attention_size, time_major=False, return_alphas=False):\n",
    "    \"\"\"\n",
    "    Attention mechanism layer which reduces RNN/Bi-RNN outputs with Attention vector.\n",
    "    The idea was proposed in the article by Z. Yang et al., \"Hierarchical Attention Networks\n",
    "     for Document Classification\", 2016: http://www.aclweb.org/anthology/N16-1174.\n",
    "    Variables notation is also inherited from the article\n",
    "    Args:\n",
    "        inputs: The Attention inputs.\n",
    "            Matches outputs of RNN/Bi-RNN layer (not final state):\n",
    "                In case of RNN, this must be RNN outputs `Tensor`:\n",
    "                    If time_major == False (default), this must be a tensor of shape:\n",
    "                        `[batch_size, max_time, cell.output_size]`.\n",
    "                    If time_major == True, this must be a tensor of shape:\n",
    "                        `[max_time, batch_size, cell.output_size]`.\n",
    "                In case of Bidirectional RNN, this must be a tuple (outputs_fw, outputs_bw) containing the forward and\n",
    "                the backward RNN outputs `Tensor`.\n",
    "                    If time_major == False (default),\n",
    "                        outputs_fw is a `Tensor` shaped:\n",
    "                        `[batch_size, max_time, cell_fw.output_size]`\n",
    "                        and outputs_bw is a `Tensor` shaped:\n",
    "                        `[batch_size, max_time, cell_bw.output_size]`.\n",
    "                    If time_major == True,\n",
    "                        outputs_fw is a `Tensor` shaped:\n",
    "                        `[max_time, batch_size, cell_fw.output_size]`\n",
    "                        and outputs_bw is a `Tensor` shaped:\n",
    "                        `[max_time, batch_size, cell_bw.output_size]`.\n",
    "        attention_size: Linear size of the Attention weights.\n",
    "        time_major: The shape format of the `inputs` Tensors.\n",
    "            If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n",
    "            If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n",
    "            Using `time_major = True` is a bit more efficient because it avoids\n",
    "            transposes at the beginning and end of the RNN calculation.  However,\n",
    "            most TensorFlow data is batch-major, so by default this function\n",
    "            accepts input and emits output in batch-major form.\n",
    "        return_alphas: Whether to return attention coefficients variable along with layer's output.\n",
    "            Used for visualization purpose.\n",
    "    Returns:\n",
    "        The Attention output `Tensor`.\n",
    "        In case of RNN, this will be a `Tensor` shaped:\n",
    "            `[batch_size, cell.output_size]`.\n",
    "        In case of Bidirectional RNN, this will be a `Tensor` shaped:\n",
    "            `[batch_size, cell_fw.output_size + cell_bw.output_size]`.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(inputs, tuple):\n",
    "        # In case of Bi-RNN, concatenate the forward and the backward RNN outputs.\n",
    "        inputs = tf.concat(inputs, 2)\n",
    "\n",
    "    if time_major:\n",
    "        # (T,B,D) => (B,T,D)\n",
    "        inputs = tf.array_ops.transpose(inputs, [1, 0, 2])\n",
    "\n",
    "    hidden_size = inputs.shape[2].value  # D value - hidden size of the RNN layer\n",
    "\n",
    "    # Trainable parameters\n",
    "    w_omega = tf.Variable(tf.random_normal([hidden_size, attention_size], stddev=0.1))\n",
    "    b_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "    u_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "\n",
    "    with tf.name_scope('v'):\n",
    "        # Applying fully connected layer with non-linear activation to each of the B*T timestamps;\n",
    "        #  the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size\n",
    "        v = tf.tanh(tf.tensordot(inputs, w_omega, axes=1) + b_omega)\n",
    "\n",
    "    # For each of the timestamps its vector of size A from `v` is reduced with `u` vector\n",
    "    vu = tf.tensordot(v, u_omega, axes=1, name='vu')  # (B,T) shape\n",
    "    alphas = K.softmax(vu)  # (B,T) shape\n",
    "\n",
    "    # Output of (Bi-)RNN is reduced with attention vector; the result has (B,D) shape\n",
    "    output = K.sum(inputs * tf.expand_dims(alphas, -1), 1)\n",
    "\n",
    "    if not return_alphas:\n",
    "        return output\n",
    "    else:\n",
    "        return output, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatibl|e with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint,\n",
    "                                 trainable = True)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint,\n",
    "                                     trainable = True)\n",
    "\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint,\n",
    "                                 trainable = True)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttLayer(Layer):\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init((self.attention_dim, )))\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "        super(AttLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # size of x :[batch_size, sel_len, attention_dim]\n",
    "        # size of u :[batch_size, attention_dim]\n",
    "        # uit = tanh(xW+b)\n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "\n",
    "        ait = K.exp(ait)\n",
    "\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = x * ait\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel():\n",
    "    word_in = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "\n",
    "    \n",
    "    word_embedding = Embedding(embeddingMatrix.shape[0],\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embeddingMatrix],\n",
    "                                #input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)(word_in)\n",
    "    \n",
    "    lstmLayer = Bidirectional(LSTM(192, dropout=0.2,return_sequences=True))(word_embedding)\n",
    "    attention_layer = AttLayer(5)(lstmLayer)\n",
    "    predictions = Dense(NUM_CLASSES, activation='sigmoid')(attention_layer)\n",
    "    \n",
    "    model = Model(inputs=word_in, outputs=predictions)\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    rmsprop = optimizers.adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=rmsprop,\n",
    "                  metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 100, 300)          4451700   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100, 384)          757248    \n",
      "_________________________________________________________________\n",
      "att_layer_1 (AttLayer)       (None, 384)               1930      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 1540      \n",
      "=================================================================\n",
      "Total params: 5,212,418\n",
      "Trainable params: 760,718\n",
      "Non-trainable params: 4,451,700\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x15db67898>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buildModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pad_sequences(sequences, sequences_char=None):\n",
    "    wordseq = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH,\n",
    "                             padding='post', truncating='post')\n",
    "    if not sequences_char:\n",
    "        return wordseq\n",
    "    \n",
    "    charseq_pre = []\n",
    "    for seq in sequences_char:\n",
    "        padded_seq = pad_sequences(seq, maxlen=MAX_CHARSEQUENCE_LENGTH,\n",
    "                                   padding='post', truncating='post')\n",
    "        charseq_pre.append(padded_seq)\n",
    "    charseq_pre = np.array(charseq_pre)\n",
    "    charseq = pad_sequences(charseq_pre, maxlen=MAX_SEQUENCE_LENGTH,\n",
    "                                 padding='post', truncating='post')\n",
    "    return [wordseq, charseq]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting k-fold cross validation...\n",
      "----------------------------------------\n",
      "Fold 1/5\n",
      "Building model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 100, 300)          4451700   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 100, 384)          757248    \n",
      "_________________________________________________________________\n",
      "att_layer_2 (AttLayer)       (None, 384)               1930      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 1540      \n",
      "=================================================================\n",
      "Total params: 5,212,418\n",
      "Trainable params: 760,718\n",
      "Non-trainable params: 4,451,700\n",
      "_________________________________________________________________\n",
      "Train on 24128 samples, validate on 6032 samples\n",
      "Epoch 1/20\n",
      "24128/24128 [==============================] - 143s 6ms/step - loss: 0.9668 - acc: 0.5938 - val_loss: 0.5605 - val_acc: 0.8014\n",
      "Epoch 2/20\n",
      "24128/24128 [==============================] - 145s 6ms/step - loss: 0.4413 - acc: 0.8480 - val_loss: 0.3332 - val_acc: 0.8876\n",
      "Epoch 3/20\n",
      " 6400/24128 [======>.......................] - ETA: 1:39 - loss: 0.3364 - acc: 0.8842"
     ]
    }
   ],
   "source": [
    "#[data_wordseq,data_charseq] = create_pad_sequences(trainSequences, trainSequences_char)\n",
    "data_wordseq = create_pad_sequences(trainSequences)\n",
    "labels = to_categorical(np.asarray(labels_pre))\n",
    "\n",
    "\n",
    "# Randomize data\n",
    "np.random.seed(10)\n",
    "# np.random.shuffle(trainIndices)\n",
    "# lim = int(0.85*len(trainSequences))\n",
    "data_wordseq = data_wordseq[trainIndices]\n",
    "#data_charseq = data_charseq[trainIndices]\n",
    "labels_train = labels[trainIndices]\n",
    "\n",
    "#data_train = [data_wordseq, data_charseq]\n",
    "data_train = data_wordseq\n",
    "\n",
    "# Perform k-fold cross validation\n",
    "metrics = {\"accuracy\" : [],\n",
    "           \"microPrecision\" : [],\n",
    "           \"microRecall\" : [],\n",
    "           \"microF1\" : []}\n",
    "\n",
    "print(\"Starting k-fold cross validation...\")\n",
    "for k in range(3):\n",
    "    print('-'*40)\n",
    "    print(\"Fold %d/%d\" % (k+1, NUM_FOLDS))\n",
    "    validationSize = int(len(labels_train)/NUM_FOLDS)\n",
    "    index1 = validationSize * k\n",
    "    index2 = validationSize * (k+1)\n",
    "\n",
    "    xTrain = np.vstack((data_train[:index1],data_train[index2:]))\n",
    "    yTrain = np.vstack((labels[:index1],labels[index2:]))\n",
    "    xVal = data_train[index1:index2]\n",
    "    yVal = labels[index1:index2]\n",
    "    print(\"Building model...\")\n",
    "    model = buildModel()\n",
    "    model.fit(xTrain,\n",
    "              yTrain, \n",
    "              validation_data=(xVal, yVal),\n",
    "              epochs=NUM_EPOCHS,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              verbose=1)\n",
    "\n",
    "    predictions = model.predict(xVal, batch_size=BATCH_SIZE)\n",
    "    accuracy, microPrecision, microRecall, microF1 = getMetrics(predictions, yVal)\n",
    "    metrics[\"accuracy\"].append(accuracy)\n",
    "    metrics[\"microPrecision\"].append(microPrecision)\n",
    "    metrics[\"microRecall\"].append(microRecall)\n",
    "    metrics[\"microF1\"].append(microF1)\n",
    "\n",
    "print(\"\\n============= Metrics =================\")\n",
    "print(\"Average Cross-Validation Accuracy : \")\n",
    "print(\"%.4f\" % (sum(metrics[\"accuracy\"])/len(metrics[\"accuracy\"])))\n",
    "print(\"Average Cross-Validation Micro Precision :\")\n",
    "print(\"%.4f\" % (sum(metrics[\"microPrecision\"])/len(metrics[\"microPrecision\"])))\n",
    "print(\"Average Cross-Validation Micro Recall :\")\n",
    "print(\"%.4f\" % (sum(metrics[\"microRecall\"])/len(metrics[\"microRecall\"])))\n",
    "print(\"Average Cross-Validation Micro F1 :\")\n",
    "print(\"%.4f\" % (sum(metrics[\"microF1\"])/len(metrics[\"microF1\"])))\n",
    "\n",
    "print(\"\\n======================================\")\n",
    "\n",
    "print(\"Retraining model on entire data to create solution file\")\n",
    "model = buildModel()\n",
    "history = model.fit(data_train,\n",
    "                    labels_train,\n",
    "                    epochs=int(NUM_EPOCHS),\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    validation_data=(data_valid, labels_valid),\n",
    "                    verbose=2)\n",
    "#model.save('EP%d_LR%de-5_LDim%d_BS%d.h5'%(NUM_EPOCHS, int(LEARNING_RATE*(10**5)), LSTM_DIM, BATCH_SIZE))\n",
    "#model = load_model('EP%d_LR%de-5_LDim%d_BS%d.h5'%(20, int(LEARNING_RATE*(10**5)), LSTM_DIM, BATCH_SIZE))\n",
    "\n",
    "print(\"Creating solution file...\")\n",
    "# testData = create_pad_sequences(testSequences, testSequences_char)\n",
    "testData = create_pad_sequences(testSequences)\n",
    "\n",
    "predictions = model.predict(testData, batch_size=BATCH_SIZE)\n",
    "predictions = predictions.argmax(axis=1)\n",
    "\n",
    "with io.open(solutionPath, \"w\", encoding=\"utf8\") as fout:\n",
    "    fout.write('\\t'.join([\"id\", \"turn1\", \"turn2\", \"turn3\", \"label\"]) + '\\n')        \n",
    "    with io.open(testDataPath, encoding=\"utf8\") as fin:\n",
    "        fin.readline()\n",
    "        for lineNum, line in enumerate(fin):\n",
    "            fout.write('\\t'.join(line.strip().split('\\t')[:4]) + '\\t')\n",
    "            fout.write(label2emotion[predictions[lineNum]] + '\\n')\n",
    "print(\"Completed. Model parameters: \")\n",
    "print(\"Learning rate : %.3f, LSTM Dim : %d, Dropout : %.3f, Batch_size : %d\" \n",
    "      % (LEARNING_RATE, LSTM_DIM, DROPOUT, BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
